{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds site map of entire domain\n",
    "from usp.tree import sitemap_tree_for_homepage\n",
    "\n",
    "#for saving and loading objects\n",
    "import pickle\n",
    "\n",
    "#for accessing url html\n",
    "import requests\n",
    "import urllib.request, urllib.parse, urllib.error\n",
    "from urllib.request import Request, urlopen\n",
    "\n",
    "#for html parsing\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#anxillary functions/methods\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "from dateutil.tz import tzutc, tzoffset\n",
    "from tqdm.notebook import tqdm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sitemap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = sitemap_tree_for_homepage('https://hongkongfp.com/');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "urls = []\n",
    "dates = []\n",
    "for page in tree.all_pages():\n",
    "    urls.append(page.url)\n",
    "    dates.append(page.last_modified)\n",
    "    \n",
    "#transpose\n",
    "zippedList =list(zip(urls,dates))\n",
    "\n",
    "#format into dataframe\n",
    "df = pd.DataFrame(zippedList, columns = ['urls' ,'date'])\n",
    "\n",
    "#drop nan for date published\n",
    "df = df.dropna()\n",
    "        \n",
    "pickle.dump(df, open(\"../Pickles/HKFP_All.p\", \"wb\" ) )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15388"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LOAD DATASET OF ALL SCRAPED URLS FROM SCMP\n",
    "with open(\"../Pickles/HKFP_All.p\", 'rb') as f:\n",
    "    df = pickle.load(f)\n",
    "          \n",
    "#reset df index\n",
    "df.reset_index(drop = True)\n",
    "\n",
    "#find articles with \"Articles\" to exclude photos/video pages\n",
    "articles_only_urls = []\n",
    "for i in df.urls:\n",
    "    if '2018' in i or '2019' in i or '2020' in i:\n",
    "        articles_only_urls.append(i)\n",
    "        \n",
    "author = []\n",
    "title = []\n",
    "date = []\n",
    "summary = []\n",
    "link = []\n",
    "broken = []\n",
    "\n",
    "len(articles_only_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4385686e76f84536808792b44c78c0cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2000.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset contains 1000 entries.\n",
      "Dataset contains 2000 entries.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#using BS4 to request each video html data\n",
    "for i in tqdm(articles_only_urls[2000:3000]):\n",
    "    try:\n",
    "        \n",
    "        #time.sleep(1) \n",
    "        req = Request(str(i), headers={'User-Agent': 'Mozilla/5.0'})\n",
    "        webpage = urlopen(req).read()\n",
    "        soup = BeautifulSoup(webpage, 'lxml')\n",
    "\n",
    "        by = soup.find(\"a\", {\"class\": \"url fn n\"})\n",
    "        t = soup.find(\"h1\", {\"class\": \"entry-title\"})\n",
    "        d = soup.find(\"time\", {\"class\": \"entry-date published\"})\n",
    "        summ = soup.find(\"div\",{\"class\":\"entry-content\"}).select_one(\"p\")\n",
    "\n",
    "\n",
    "        if by and t and d:\n",
    "            author.append(by.text)\n",
    "            title.append(t.text.strip())\n",
    "            date.append(d.text)\n",
    "            summary.append(summ.text.strip().replace(u'\\xa0', u' '))\n",
    "            link.append(i)\n",
    "\n",
    "            if len(link)% 1000 == 0:\n",
    "                print(\"Dataset contains \" + str(len(link)) + \" entries.\")\n",
    "    except:\n",
    "        print(\"URL number \" + str(urls.index(i)) + \" is broken: \" + i )\n",
    "        broken.append(i)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(list(zip(title, author, date, summary, link)), \n",
    "               columns =['Title', 'Author', 'Date', 'Summary','Link']) \n",
    "    \n",
    "\n",
    "with open('../Pickles/HkfpDF.pkl', 'wb') as f:\n",
    "    pickle.dump(df, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle(r'../Pickles/SCMP_Opinions.pkl')\n",
    "SCMPEditorial = df[df['Author'] == 'SCMP Editorial']\n",
    "SCMPEditorial = SCMPEditorial.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In Pictures: Hundreds protest against Tuen Mun’s ‘dancing aunties’ as police deploy pepper spray',\n",
       " 'Traces of coronavirus detected in Hong Kong patient’s dog but no evidence of infection, says gov’t dept.',\n",
       " 'China censors report about how authorities hid coronavirus genome sequence test results for 14 days',\n",
       " 'Community film screenings across Hong Kong mark 9 months since first anti-extradition law protest clashes',\n",
       " 'Hundreds commemorate 15-year-old Hong Kong student who died last September',\n",
       " 'In Pictures: New Hong Kong water park to make a splash in 2018',\n",
       " '‘I want to die for HK’ football coach to stay until 2018',\n",
       " 'China’s LeEco inks exclusive deal to show 2018 football World Cup in Hong Kong',\n",
       " 'HKFP Lens: ‘We are Hong Kong’ – memories of the 2018 World Cup qualifiers',\n",
       " 'China’s population to grow 45 million by 2020: Plan',\n",
       " 'Trial for Mong Kok protesters Ray Wong and Edward Leung to begin in 2018']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
