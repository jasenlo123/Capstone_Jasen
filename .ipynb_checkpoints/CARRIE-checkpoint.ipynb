{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finds site map of entire domain\n",
    "from usp.tree import sitemap_tree_for_homepage\n",
    "\n",
    "#for saving and loading objects\n",
    "import pickle\n",
    "\n",
    "#for accessing url html\n",
    "import requests\n",
    "\n",
    "#for html parsing\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#anxillary functions/methods\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import datetime\n",
    "from dateutil.tz import tzutc, tzoffset\n",
    "\n",
    "from bs4 import BeautifulSoup \n",
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urlsplit\n",
    "from collections import deque\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://scrapethissite.com/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a queue of urls to be crawled next\n",
    "new_urls = deque([url])\n",
    "\n",
    "# a set of urls that we have already processed \n",
    "processed_urls = set()\n",
    "# a set of domains inside the target website\n",
    "local_urls = set()\n",
    "# a set of domains outside the target website\n",
    "foreign_urls = set()\n",
    "# a set of broken urls\n",
    "broken_urls = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process urls one by one until we exhaust the queue\n",
    "while len(new_urls):    \n",
    "    # move url from the queue to processed url set    \n",
    "    url = new_urls.popleft()    \n",
    "    processed_urls.add(url)    \n",
    "    # print the current url    \n",
    "    print(\"Processing %s\" % url)\n",
    "    try:    \n",
    "        response = requests.get(url)\n",
    "    except(requests.exceptions.MissingSchema, requests.exceptions.ConnectionError, requests.exceptions.InvalidURL, requests.exceptions.InvalidSchema):    \n",
    "    # add broken urls to itâ€™s own set, then continue    \n",
    "        broken_urls.add(url)     \n",
    "        continue\n",
    "        \n",
    "    # extract base url to resolve relative links\n",
    "    parts = urlsplit(url)\n",
    "    base = \"{0.netloc}\".format(parts)\n",
    "    strip_base = base.replace(\"www.\", \"\")\n",
    "    base_url = \"{0.scheme}://{0.netloc}\".format(parts)\n",
    "    path = url[:url.rfind('/')+1] if '/' in parts.path else url\n",
    "\n",
    "\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    for link in soup.find_all('a'):  \n",
    "        \n",
    "        # extract link url from the anchor    \n",
    "        anchor = link.attrs[\"href\"] if \"href\" in link.attrs else ''\n",
    "        print(anchor)\n",
    "        print(  )\n",
    "        \n",
    "        if anchor.startswith('/'):        \n",
    "            local_link = base_url + anchor        \n",
    "            local_urls.add(local_link)    \n",
    "        elif strip_base in anchor:        \n",
    "            local_urls.add(anchor)    \n",
    "        elif not anchor.startswith('http'):        \n",
    "            local_link = path + anchor        \n",
    "            local_urls.add(local_link)    \n",
    "        else:       \n",
    "            foreign_urls.add(anchor)\n",
    "\n",
    "    for i in local_urls:    \n",
    "        if not i in new_urls and not i in processed_urls:        \n",
    "            new_urls.append(i)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'https://scrapethissite.com/',\n",
       " 'https://scrapethissite.com/faq/',\n",
       " 'https://scrapethissite.com/lessons/',\n",
       " 'https://scrapethissite.com/login/',\n",
       " 'https://scrapethissite.com/pages/',\n",
       " 'https://scrapethissite.com/pages/advanced/',\n",
       " 'https://scrapethissite.com/pages/advanced/?gotcha=csrf',\n",
       " 'https://scrapethissite.com/pages/advanced/?gotcha=headers',\n",
       " 'https://scrapethissite.com/pages/advanced/?gotcha=login',\n",
       " 'https://scrapethissite.com/pages/ajax-javascript/',\n",
       " 'https://scrapethissite.com/pages/ajax-javascript/#',\n",
       " 'https://scrapethissite.com/pages/forms/',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=1',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=10',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=11',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=12',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=13',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=14',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=15',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=16',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=17',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=18',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=19',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=2',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=20',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=21',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=22',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=23',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=24',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=3',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=4',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=5',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=6',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=7',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=8',\n",
       " 'https://scrapethissite.com/pages/forms/?page_num=9',\n",
       " 'https://scrapethissite.com/pages/frames/',\n",
       " 'https://scrapethissite.com/pages/simple/',\n",
       " 'https://scrapethissite.com/robots.txt'}"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
